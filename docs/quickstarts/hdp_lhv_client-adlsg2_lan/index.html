<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Hortonworks (HDP) with Live Hive &amp; Client to ADLS Gen2 with Live Analytics · WANdisco docs</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="_THIS GUIDE IS WORK IN PROGRESS, PLEASE DO NOT FOLLOW ANYTHING HERE UNTIL THIS WARNING IS REMOVED_"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Hortonworks (HDP) with Live Hive &amp; Client to ADLS Gen2 with Live Analytics · WANdisco docs"/><meta property="og:type" content="website"/><meta property="og:url" content="https://wandisco.github.io/wandisco-documentation/ https://wandisco.github.io/wandisco-documentation/"/><meta property="og:description" content="_THIS GUIDE IS WORK IN PROGRESS, PLEASE DO NOT FOLLOW ANYTHING HERE UNTIL THIS WARNING IS REMOVED_"/><meta property="og:image" content="https://wandisco.github.io/wandisco-documentation/ https://wandisco.github.io/wandisco-documentation/img/undraw_online.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://wandisco.github.io/wandisco-documentation/ https://wandisco.github.io/wandisco-documentation/img/undraw_tweetstorm.svg"/><link rel="shortcut icon" href="https://wandisco.github.io/wandisco-documentation/img/favicon.png"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script src="https://wandisco.github.io/wandisco-documentation/js/scrollSpy.js"></script><link rel="stylesheet" href="https://wandisco.github.io/wandisco-documentation/css/main.css"/><script src="https://wandisco.github.io/wandisco-documentation/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="https://wandisco.github.io/wandisco-documentation/"><img class="logo" src="https://wandisco.github.io/wandisco-documentation/img/favicon.png" alt="WANdisco docs"/><h2 class="headerTitleWithLogo">WANdisco docs</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="https://wandisco.github.io/wandisco-documentation/docs/quickstarts/quickstart-config" target="_self">Quickstarts</a></li><li class=""><a href="https://wandisco.github.io/wandisco-documentation/docs/docs/doc1" target="_self">Docs</a></li><li class=""><a href="https://wandisco.github.io/wandisco-documentation/docs/glossary/a" target="_self">Glossary</a></li><li class=""><a href="https://wandisco.github.io/wandisco-documentation/docs/api/api" target="_self">API</a></li><li class=""><a href="https://wandisco.github.io/wandisco-documentation/help" target="_self">Help</a></li><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search" title="Search"/></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="container mainContainer"><div class="wrapper"><div class="post"><header class="postHeader"><h1 class="postHeaderTitle">Hortonworks (HDP) with Live Hive &amp; Client to ADLS Gen2 with Live Analytics</h1></header><article><div><span><p><em>THIS GUIDE IS WORK IN PROGRESS, PLEASE DO NOT FOLLOW ANYTHING HERE UNTIL THIS WARNING IS REMOVED</em></p>
<p>Use this quickstart if you want to configure Fusion to connect to Hortonworks (HDP) and ADLS Gen2 storage/Databricks cluster. This guide will also include Live Hive on the HDP cluster, and Live Analytics on the ADLS Gen2/Databricks cluster.</p>
<p>Please see the <a href="https://wandisco.github.io/wandisco-documentation/docs/troubleshooting/useful_info">Useful information</a> section for additional commands and help.</p>
<h2><a class="anchor" aria-hidden="true" id="prerequisites"></a><a href="#prerequisites" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Prerequisites</h2>
<p>To complete this lab exercise, you will need:</p>
<ul>
<li>Azure VM instance set up and running, with root access available (instructions were tested on CentOS-based 7.7).
<ul>
<li><p>(TBC) Minimum size VM recommendation = <strong>Standard A4m v2 (4 vcpus, 32 GiB memory).</strong></p>
<p>A minimum of 100GB storage is required for the <code>/var</code> partition.</p></li>
<li><p><a href="https://docs.docker.com/install/">Docker</a> (v19.03.3 or higher), <a href="https://docs.docker.com/compose/install/">Docker Compose</a> (v1.24.1 or higher), and <a href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git">Git</a> installed on instance.</p></li>
<li><p><code>wget</code>, <code>tar</code> and <code>unzip</code> utilities installed on the instance, including a CLI text editor (such as <code>vi</code>).</p></li>
<li><p>Java v1.8 installed on the instance (run <code>yum install -y java-1.8.0-openjdk.x86_64</code> on the VM).</p></li>
<li><p>Iptables and selinux are disabled.</p></li>
</ul></li>
<li>Administrator credentials for the HDP Ambari Manager and root access via terminal.</li>
<li>Network connectivity to the Ambari Manager and NameNode.</li>
<li>Credentials for accessing the Data Lake Storage Gen2 and Databricks cluster.</li>
<li>Network connectivity to the Data Lake Storage Gen2 and Databricks cluster.</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="guidance"></a><a href="#guidance" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Guidance</h2>
<p>All the commands within this guidance should be run as <strong>root</strong> user. To switch to this user, type <code>sudo -i</code> in the command line when logged in as the default Azure user (this will have been set during creation of the VM).</p>
<h3><a class="anchor" aria-hidden="true" id="initial-setup"></a><a href="#initial-setup" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Initial Setup</h3>
<ol>
<li><p>Clone the Fusion docker repository to your Azure VM instance:</p>
<p><code>git clone -b features/livehive-merge https://github.com/WANdisco/fusion-docker-compose.git</code></p></li>
<li><p>Change to the repository directory:</p>
<p><code>cd fusion-docker-compose</code></p></li>
<li><p>Run the setup script:</p>
<p><code>./setup-env.sh</code></p></li>
<li><p>Follow the prompts to configure your zones, see the next section below for guidance on this.</p></li>
</ol>
<h3><a class="anchor" aria-hidden="true" id="setup-prompts"></a><a href="#setup-prompts" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Setup prompts</h3>
<p><em>Zone type</em></p>
<ul>
<li>For the purposes of this quickstart, please enter <code>hdp</code> for the first zone type, and <code>adls2</code> for the second zone type.</li>
</ul>
<p><em>Zone name</em></p>
<ul>
<li>If defining a zone name, please note that each zone must have a different name (i.e. they cannot match). Otherwise, press enter to leave as default.</li>
</ul>
<p><em>Licenses</em></p>
<ul>
<li>Trial licenses will last 30 days and are limited to 1TB of replicated data. Press enter to leave as default trial license.</li>
</ul>
<p><em>Docker hostname</em></p>
<ul>
<li>For the purposes of this quickstart, this can be changed to the IP address of your docker host.</li>
</ul>
<p><em>Example entries for HDP</em></p>
<ul>
<li><p>HDP version: <code>2.6.5</code></p></li>
<li><p>Hadoop NameNode IP/hostname: <code>namenode.example.com</code> - The value will be the hostname defined in the <code>fs.defaultFS</code> property in the HDFS config, but not including the <code>hdfs://</code> prefix.</p></li>
<li><p>NameNode port: <code>8020</code> - The value will be the port defined in the <code>fs.defaultFS</code> property in the HDFS config.</p></li>
<li><p>NameNode Service Name: <code>&lt;namenode_hostname&gt;:8020</code> - Press adjust this property so that it references the HDP cluster's NameNode hostname on port 8020.</p></li>
</ul>
<p><em>Example entries for Live Hive</em></p>
<ul>
<li><p>Enter <code>livehive</code> for the HDP zone when prompted to select a plugin.</p></li>
<li><p>Hive Metastore hostname: <code>metastore.hostname.com</code> - The HDP cluster's Hive Metastore hostname, can be seen by hovering over the Hive Metastore in the Hive summary page.</p></li>
<li><p>Hive Metastore port: <code>9083</code> - This value can be left as default.</p></li>
</ul>
<p><em>Example entries for ADLS Gen2</em></p>
<ul>
<li><p>HDI version: <code>3.6</code></p></li>
<li><p>Storage account: <code>adlsg2storage</code></p></li>
<li><p>Storage container: <code>fusionreplication</code></p></li>
<li><p>Account key: <code>KEY_1_STRING</code> - the Primary Access Key is now referred to as &quot;Key1&quot; in Microsoft’s documentation. You can get the Access Key from the Microsoft Azure storage account under the <strong>Access Keys</strong> section.</p></li>
<li><p>default FS: <code>abfss://fusionreplication@adlsg2storage.dfs.core.windows.net/</code> - press enter for the default value.</p></li>
<li><p>underlying FS: <code>abfs://fusionreplication@adlsg2storage.dfs.core.windows.net/</code> - press enter for the default value.</p></li>
<li><p>Enter <code>NONE</code> for the adls2 zone when prompted to select a plugin.</p></li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="startup"></a><a href="#startup" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Startup</h3>
<p>After all the prompts have been completed, you will be able to start the containers.</p>
<ol>
<li><p>Ensure that Docker is started:</p>
<p><code>systemctl status docker</code></p>
<p>If not, start the Docker service:</p>
<p><code>systemctl start docker</code></p></li>
<li><p>Start the Fusion containers with:</p>
<p><code>docker-compose up -d</code></p></li>
</ol>
<h3><a class="anchor" aria-hidden="true" id="live-hive-config-changes"></a><a href="#live-hive-config-changes" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Live Hive config changes</h3>
<ol>
<li><p>Log into one of the containers for the HDP zone.</p>
<p><code>docker exec -it fusion-docker-compose_fusion-ui-server-hdp_1 bash</code></p></li>
<li><p>Add an additional property to the Live Hive config:</p>
<p><code>vi /etc/wandisco/fusion/plugins/hive/live-hive-site.xml</code></p>
<p>Add the following property and value below:</p>
<pre><code class="hljs css language-json">  &lt;property&gt;
    &lt;name&gt;live.hive.cluster.delegation.token.delayed.removal.interval.in.seconds&lt;/name&gt;
    &lt;value&gt;5&lt;/value&gt;
  &lt;/property&gt;
</code></pre>
<p>Once complete, save and quit the file (e.g. <code>:wq!</code>).</p></li>
<li><p>Exit back into the docker host and restart the Fusion containers so that the configuration changes are picked up.</p>
<p><code>exit</code></p>
<p><code>docker-compose restart</code></p></li>
<li><p>Log into the Fusion UI for the HDP zone, and activate the Live Hive plugin.</p>
<p><code>http://&lt;docker_hostname/IP&gt;:8083</code></p>
<p>Username: <code>admin</code>
Password: <code>admin</code></p>
<p>Proceed to the Settings tab and select the <em>Live Hive: Plugin Activation</em> option on the left-hand panel.</p>
<p>Click on the <em>Activate</em> option.</p></li>
</ol>
<h3><a class="anchor" aria-hidden="true" id="install-fusion-client-on-hdp-nodes"></a><a href="#install-fusion-client-on-hdp-nodes" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Install Fusion Client on HDP nodes</h3>
<ol>
<li><p>Log into the Ambari Server via a terminal session.</p></li>
<li><p>Download the Fusion Client stack package located here:</p>
<p><code>wget http://&lt;docker_hostname/IP&gt;:8083/ui/downloads/stack_packages/fusion-hcfs-hdp-2.6.5-2.14.2.1.stack.tar.gz</code></p></li>
<li><p>Decompress the stack to the Ambari services directory.</p>
<p><code>tar -xf fusion-hcfs-hdp-2.6.5-2.14.2.1.stack.tar.gz -C /var/lib/ambari-server/resources/stacks/HDP/2.6/services/</code></p></li>
<li><p>Delete the compressed file afterwards.</p>
<p><code>rm -f fusion-hcfs-hdp-2.6.5-2.14.2.1.stack.tar.gz</code></p></li>
<li><p>Restart the Ambari Server so that the new stack will be available to install.</p>
<p><code>ambari-server restart</code></p></li>
<li><p>Log into the Ambari UI once it is available after the restart.</p></li>
<li><p>In the Dashboard, select to <strong>Add Service</strong> in the Actions.</p></li>
<li><p>Select the <strong>Wandisco Fusion Client</strong> and click Next.</p></li>
<li><p>Ensure the <strong>Wandisco Fusion Client</strong> is assigned to all nodes. Click Next once confirmed.</p></li>
<li><p>On the <strong>Customize Services</strong> page, insert the URL for the Wandisco Fusion UI for the HDP zone - <code>http://&lt;dockerip&gt;:8083</code>. Click Next once complete.</p>
<p><em>Please ignore any Recommended Changes if they are suggested by Ambari. This can be done by unticking the checkbox left of the Property column header, and clicking OK.</em></p></li>
<li><p>On the <strong>Review</strong> page, click Deploy. After the <strong>Install, Start and Test</strong> operation is complete, click Next.</p></li>
<li><p>On the <strong>Summary</strong> page, click Complete.</p></li>
</ol>
<h3><a class="anchor" aria-hidden="true" id="install-the-live-hiveserver2-template-on-the-hdp-cluster"></a><a href="#install-the-live-hiveserver2-template-on-the-hdp-cluster" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Install the Live Hiveserver2 template on the HDP cluster</h3>
<ol>
<li><p>Log into the Ambari Server via a terminal session.</p></li>
<li><p>Download the Live Hiveserver2 template stack from the docker host.</p>
<p><code>wget http://&lt;docker_hostname/IP&gt;:8083/ui/downloads/core_plugins/live-hive/stack_packages/live-hiveserver2-template-5.0.0.1.stack.tar.gz</code></p></li>
<li><p>Decompress the stack to the Ambari services directory.</p>
<p><code>tar -xf live-hiveserver2-template-5.0.0.1.stack.tar.gz -C /var/lib/ambari-server/resources/stacks/HDP/2.6/services/</code></p></li>
<li><p>Delete the compressed file afterwards.</p>
<p><code>rm -f live-hiveserver2-template-5.0.0.1.stack.tar.gz</code></p></li>
<li><p>Restart the Ambari Server so that the new stack will be available to install.</p>
<p><code>ambari-server restart</code></p></li>
<li><p>Log into the Ambari UI once it is available after the restart.</p></li>
<li><p>In the Dashboard, select to <strong>Add Service</strong> in the Actions.</p></li>
<li><p>Select the <strong>Live Hiveserver2 Template</strong> and click Next.</p></li>
<li><p>Ensure the <strong>Live Hiveserver2 Template Master</strong> is assigned to the host on which the <strong>Hiveserver2</strong> component is installed. Click Next once confirmed.</p></li>
<li><p>On the <strong>Customize Services</strong> page, click Next.</p>
<p><em>Please ignore any Recommended Changes if they are suggested by Ambari. This can be done by unticking the checkbox left of the Property column header, and clicking OK.</em></p></li>
<li><p>On the <strong>Review</strong> page, click Deploy. After the <strong>Install, Start and Test</strong> operation is complete, click Next.</p></li>
<li><p>On the <strong>Summary</strong> page, click Complete.</p></li>
</ol>
<h3><a class="anchor" aria-hidden="true" id="activate-fusion-client-on-the-hdp-cluster"></a><a href="#activate-fusion-client-on-the-hdp-cluster" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Activate Fusion Client on the HDP cluster</h3>
<ol>
<li><p>Log into the Ambari UI for the HDP cluster.</p></li>
<li><p>Add the following properties to the HDFS config so that the Fusion Client is used on the cluster.</p>
<p><strong>HDFS -&gt; Configs -&gt; Advanced -&gt; Custom core-site</strong></p>
<p>Click <strong>Add Property ...</strong> and add the following below in <strong>Bulk property add mode</strong>.</p>
<pre><code class="hljs css language-json">fs.fusion.underlyingFs=hdfs://&lt;namenode_hostname&gt;:8020
fs.hdfs.impl=com.wandisco.fs.client.FusionHdfs
fusion.client.ssl.enabled=false
fusion.http.authentication.enabled=false
fusion.http.authorization.enabled=false
fusion.server=&lt;docker_hostname/IP&gt;:8023
hadoop.proxyuser.hdfs.hosts=&lt;docker_hostname/IP&gt;
hadoop.proxyuser.hdfs.groups=*
</code></pre>
<p>Please ensure to adjust the <code>&lt;namenode_hostname&gt;</code> and <code>&lt;docker_hostname/IP&gt;</code> values to your environment.</p>
<p>Select <strong>Add</strong> and then <strong>Save</strong> the HDFS config.</p></li>
<li><p>Adjust the following property in YARN.</p>
<p><strong>YARN -&gt; Configs -&gt; Advanced -&gt; Advanced yarn-log4j</strong></p>
<p>Add the following two new lines to the bottom of the text window.</p>
<pre><code class="hljs css language-json">log4j.logger.com.wandisco.fusion.client.BypassConfiguration=OFF
log4j.logger.com.wandisco.fs.client=OFF
</code></pre>
<p><strong>Save</strong> the YARN config afterwards.</p></li>
<li><p>Adjust the following property in MapReduce2.</p>
<p><strong>MapReduce2 -&gt; Configs -&gt; Filter for &quot;mapreduce.application.classpath&quot;</strong></p>
<p>Append the following onto the very end of the property:</p>
<p><code>:/opt/wandisco/fusion/client/lib/*</code></p>
<p>The final value should look similar to below:</p>
<pre><code class="hljs css language-json">$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/${hdp.version}/hadoop/lib/hadoop-lzo-0.6.0.${hdp.version}.jar:/etc/hadoop/conf/secure:/usr/hdp/current/ext/hadoop/*:/opt/wandisco/fusion/client/lib/*
</code></pre>
<p><strong>Save</strong> the MapReduce2 config afterwards.</p></li>
<li><p>Adjust the following property in Tez.</p>
<p><strong>Tez -&gt; Configs -&gt; Filter for &quot;tez.cluster.additional.classpath.prefix&quot;</strong></p>
<p>Append the following onto the very end of the property:</p>
<p><code>:/opt/wandisco/fusion/client/lib/*</code></p>
<p>The final value should look similar to below:</p>
<pre><code class="hljs css language-json">/usr/hdp/${hdp.version}/hadoop/lib/hadoop-lzo-0.6.0.${hdp.version}.jar:/etc/hadoop/conf/secure:/opt/wandisco/fusion/client/lib/*
</code></pre>
<p><strong>Save</strong> the Tez config afterwards.</p></li>
</ol>
<h3><a class="anchor" aria-hidden="true" id="activate-live-hive-proxy-on-the-hdp-cluster"></a><a href="#activate-live-hive-proxy-on-the-hdp-cluster" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Activate Live Hive Proxy on the HDP cluster</h3>
<ol>
<li><p>Log into the Ambari UI for the HDP cluster.</p></li>
<li><p>Adjust two properties in the Hive config so that it references the Live Hive Proxy.</p>
<p><strong>Hive -&gt; Configs -&gt; Filter for &quot;hive.metastore.uris&quot;</strong></p>
<p>Adjust the value of <code>hive.metastore.uris</code> in the following sub-sections:</p>
<p><em>General</em></p>
<pre><code class="hljs css language-json">thrift://&lt;docker_IP_address&gt;:9083
</code></pre>
<p><em>Advanced webhcat-site</em></p>
<pre><code class="hljs css language-json">hive.metastore.local=false,hive.metastore.uris=thrift://&lt;docker_IP_address&gt;:9083,hive.metastore.sasl.enabled=false
</code></pre></li>
<li><p>Adjust a property so that the Live Hive Proxy will handle both data and metadata changes.</p>
<p><strong>Hive -&gt; Configs -&gt; Filter for &quot;hive-env template&quot;</strong></p>
<p>Add the following three new lines to the bottom of the text window.</p>
<pre><code class="hljs css language-json">if [ "$SERVICE" = "metastore" ]; then
  export FUSION_REPLICATION_DISABLED=true # Set by WANdisco for use with live hive proxy
fi
</code></pre></li>
<li><p><strong>Save</strong> the Hive config after making these adjustments.</p></li>
</ol>
<h3><a class="anchor" aria-hidden="true" id="restart-required-services"></a><a href="#restart-required-services" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Restart required services</h3>
<ol>
<li><p>Restart the <strong>HDFS</strong>, <strong>YARN</strong>, <strong>MapReduce2</strong>, <strong>Tez</strong> and <strong>Hive</strong> services in that order.</p>
<p>If any other services are designated with a stale configuration, please restart them as well.</p></li>
<li><p>After all the HDP cluster services have finished restarting, log onto the docker host and restart the Fusion containers:</p>
<p><code>docker-compose restart</code></p></li>
</ol>
<h3><a class="anchor" aria-hidden="true" id="setup-databricks-on-adls-gen2-zone"></a><a href="#setup-databricks-on-adls-gen2-zone" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Setup Databricks on ADLS Gen2 zone</h3>
<ol>
<li><p>Log into the Fusion UI for the ADLS Gen2 zone.</p>
<p><code>http://&lt;docker_hostname/IP&gt;:8583</code></p>
<p>Username: <code>admin</code>
Password: <code>admin</code></p></li>
<li><p>Enter the Databricks Configuration details on the Settings page.</p>
<p><strong>Fusion UI -&gt; Settings -&gt; Databricks: Configuration</strong></p>
<p><em>Examples for Databricks details</em></p>
<ul>
<li><p>Databricks Service Address: <code>westeurope.azuredatabricks.net</code></p></li>
<li><p>Bearer Token: <code>dapicd7689jkb25473c765ghty78bb299a83</code></p></li>
<li><p>Databricks Cluster ID: <code>2233-255452-boned277</code></p></li>
<li><p>Unique JDBC HTTP path: <code>sql/protocolv1/o/6987013384345789/2233-255452-boned277</code></p></li>
</ul>
<p><em>Ensure to change the above examples to match your Databricks details.</em></p>
<p>Click <strong>Update</strong> once complete.</p></li>
<li><p>Log into one of the containers for the ADLS Gen2 zone.</p>
<p><code>ssh &lt;docker_hostname/IP&gt;</code></p>
<p><code>cd fusion-docker-compose</code></p>
<p>You will first need to obtain the name of a suitable container, this can be done by running the command below.</p>
<p><code>docker-compose ps</code> <em>- obtain list of container names.</em></p>
<p>Utilise a container name from the ADLS Gen2 zone in the command below, for example, <code>fusion-docker-compose_fusion-ui-server-adls2_1</code>.</p>
<p><code>docker exec -u root -it fusion-docker-compose_fusion-ui-server-adls2_1 bash</code></p></li>
<li><p>Upload the Live Analytics &quot;datatransformer&quot; jar using a curl command.</p>
<p><em>Example</em></p>
<ul>
<li>Bearer Token: <code>dapicd7689jkb25473c765ghty78bb299a83</code></li>
<li>Databricks Service Address: <code>westeurope.azuredatabricks.net</code></li>
</ul>
<p><code>curl -v -H &quot;Authorization: Bearer dapicd7689jkb25473c765ghty78bb299a83&quot; -F contents=@/opt/wandisco/fusion/plugins/databricks/live-analytics-databricks-etl-5.0.0.0.jar -F path=&quot;/datatransformer.jar&quot; https://westeurope.azuredatabricks.net/api/2.0/dbfs/put</code></p>
<p>You will need to adjust the command so that your Bearer Token and Databricks Service Address is referenced.</p></li>
<li><p>Log into the Azure portal and Launch Workspace for your Databricks cluster.</p></li>
<li><p>On the left-hand panel, select <strong>Clusters</strong> and then select your interactive cluster.</p></li>
<li><p>Click on the <strong>Libraries</strong> tab, and select the option to <strong>Install New</strong>.</p></li>
<li><p>Select the following options for the Install Library prompt:</p>
<ul>
<li><p>Library Source = <code>DBFS</code></p></li>
<li><p>Library Type = <code>Jar</code></p></li>
<li><p>File Path = <code>dbfs:/datatransformer.jar</code></p></li>
</ul></li>
<li><p>Select <strong>Install</strong> once the details are entered. Wait for the <strong>Status</strong> of the jar to display as <strong>Installed</strong> before continuing.</p></li>
</ol>
<h2><a class="anchor" aria-hidden="true" id="replication"></a><a href="#replication" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Replication</h2>
<p>In this section, follow the steps detailed to perform live replication of HCFS data and Hive metadata from the HDP cluster to the Azure Databricks cluster.</p>
<h3><a class="anchor" aria-hidden="true" id="create-replication-rules"></a><a href="#create-replication-rules" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Create replication rules</h3>
<ol>
<li><p>Log into the Fusion UI for the HDP zone.</p>
<p><code>http://&lt;docker_hostname/IP&gt;:8083</code></p>
<p>Username: <code>admin</code>
Password: <code>admin</code></p></li>
<li><p>Enter the Replication tab, and select to <strong>+ Create</strong> a replication rule.</p></li>
<li><p>Create a new HCFS rule using the UI with the following properties:</p>
<ul>
<li><p>Type = <code>HCFS</code></p></li>
<li><p>Zones = <code>adls2, hdp</code> <em>- Leave as default.</em></p></li>
<li><p>Priority Zone = <code>hdp</code> <em>- Leave as default.</em></p></li>
<li><p>Rule Name = <code>warehouse</code></p></li>
<li><p>Path for adls2 = <code>/apps/hive/warehouse</code></p></li>
<li><p>Path for hdp = <code>/apps/hive/warehouse</code></p></li>
</ul>
<p>Click <strong>Add</strong> after entering the Rule Name and Paths.</p>
<ul>
<li>Advanced Options: Preserve Origin Block Size = <code>true</code> <em>- click the checkbox to set this to true.</em></li>
</ul>
<p>Click <strong>Create rules (1)</strong> once complete.</p></li>
<li><p>Create a new Hive rule using the UI with the following properties:</p>
<p>On the Replication tab, select to <strong>+ Create</strong> a replication rule again.</p>
<ul>
<li><p>Type = <code>Hive</code></p></li>
<li><p>Database name = <code>test*</code></p></li>
<li><p>Table name = <code>*</code></p></li>
<li><p>Description = <code>testing</code> <em>- this field is optional</em></p></li>
</ul>
<p>Click <strong>Create rule</strong> once complete.</p></li>
<li><p>Both rules should now display on the <strong>Replication</strong> tab in the Fusion UI.</p></li>
</ol>
<h3><a class="anchor" aria-hidden="true" id="test-replication"></a><a href="#test-replication" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Test replication</h3>
<p>Prior to performing these tasks, the Databricks cluster must be in a <strong>running</strong> state. Please access the Azure portal and check the status of the cluster. If it is not running, select to start the cluster and wait until it is <strong>running</strong> before continuing.</p>
<ol>
<li><p>Log into a HDP cluster node with the Hive client available.</p>
<p>You can confirm the Hive client is installed by switching to the <code>hdfs</code> user and running <code>hive</code> on the command line.</p>
<p><code>ssh &lt;hdp-cluster-node&gt;</code></p>
<p><code>su - hdfs</code></p>
<p><code>hive</code></p>
<p>After running the Hive command as the <code>hdfs</code> user, you will now be inside a Hive interactive session.</p></li>
<li><p>Create a database to use that will match the regex for the Hive replication rule created earlier in the Fusion UI.</p>
<p><code>hive&gt; create database test01;</code></p></li>
<li><p>Create a table inside of the database.</p>
<p><code>hive&gt; use test01;</code></p>
<p><code>hive&gt; create table table01(id int, name string) stored as ORC;</code></p></li>
<li><p>Insert data inside of the table.</p>
<p><code>hive&gt; insert into table01 values (1,'words');</code></p>
<p>This will now launch a Hive job that will insert the data values provided in this example. If this is successful, you will see <strong>SUCCEEDED</strong> written in the STATUS column.</p></li>
</ol>
<h3><a class="anchor" aria-hidden="true" id="verify-replication"></a><a href="#verify-replication" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Verify replication</h3>
<ol>
<li><p>To verify the data values inside of the table on the <strong>HDP</strong> zone, run the command below when still logged into the Hive interactive session:</p>
<p><code>hive&gt; select * from table01;</code></p>
<p>The output will be similar to that of below:</p>
<pre><code class="hljs css language-json">OK
1       words
Time taken: X seconds, Fetched: 1 row(s)
</code></pre></li>
<li><p>To verify the data has replicated to the ADLS Gen2 zone and Databricks cluster, access the Azure portal and and Launch Workspace for your Databricks cluster (if not already opened).</p></li>
<li><p>On the left-hand panel, select <strong>Data</strong> and then select the database created for this test (i.e. <code>test01</code>).</p></li>
<li><p>In the <em>Tables</em> list, select the table created for this test (i.e. <code>table01</code>).</p></li>
<li><p>Wait for the table details to be loaded, and verify that the Schema and Sample Data match that seen in the HDP zone.</p>
<p><strong>Schema</strong></p>
<pre><code class="hljs css language-json">col_name   data_type
id         int
name       string
</code></pre>
<p><strong>Sample Data</strong></p>
<pre><code class="hljs css language-json">id         name
1          words
</code></pre></li>
</ol>
<h2><a class="anchor" aria-hidden="true" id="troubleshooting"></a><a href="#troubleshooting" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Troubleshooting</h2>
<h3><a class="anchor" aria-hidden="true" id="error-relating-to-system_dbus_socket"></a><a href="#error-relating-to-system_dbus_socket" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Error relating to 'system_dbus_socket'</h3>
<p>If encountering a <code>system_dbus_socket</code> error when attempting to start containers, run the following commands below on the docker host:</p>
<pre><code class="hljs css language-json">mkdir -p /run /run/lock
mv /var/run/* /run/
mv /var/lock/* /run/lock/
rm -rf /var/run /var/lock
ln -s /run /var/run
ln -s /run/lock /var/lock
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="error-relating-to-connection-refused-after-starting-fusion-for-the-first-time"></a><a href="#error-relating-to-connection-refused-after-starting-fusion-for-the-first-time" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Error relating to 'connection refused' after starting Fusion for the first time</h3>
<p>You may see the following error occur when running <code>docker-compose up -d</code> for the first time inside the fusion-docker-compose repository:</p>
<pre><code class="hljs css language-json">ERROR: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on [::1]:53: read udp [::1]:52155-&gt;[::1]:53: read: connection refused
</code></pre>
<p>If encountering this error, run the <code>docker-compose up -d</code> command again, and this should initiate the download of the docker images.</p>
<h3><a class="anchor" aria-hidden="true" id="fusion-zones-not-inducted-together-after-initial-start-up"></a><a href="#fusion-zones-not-inducted-together-after-initial-start-up" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Fusion zones not inducted together after initial start-up</h3>
<p>If the Fusion zones are not inducted together after starting Fusion for the first time (<code>docker-compose up -d</code>), you can simply run the same command again to start the induction container:</p>
<p><code>docker-compose up -d</code></p>
<h2><a class="anchor" aria-hidden="true" id="advanced-options"></a><a href="#advanced-options" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Advanced options</h2>
<ul>
<li>This guide does not currently offer configuration of Fusion to a <strong>Kerberized</strong> HDP cluster.</li>
<li>This guide does not currently offer configuration of Fusion to a NameNode HA HDP cluster.</li>
</ul>
<p>Please contact <a href="https://wandisco.com/contact">WANdisco</a> for further information on Fusion with docker.</p>
</span></div></article></div><div class="docs-prevnext"></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#prerequisites">Prerequisites</a></li><li><a href="#guidance">Guidance</a><ul class="toc-headings"><li><a href="#initial-setup">Initial Setup</a></li><li><a href="#setup-prompts">Setup prompts</a></li><li><a href="#startup">Startup</a></li><li><a href="#live-hive-config-changes">Live Hive config changes</a></li><li><a href="#install-fusion-client-on-hdp-nodes">Install Fusion Client on HDP nodes</a></li><li><a href="#install-the-live-hiveserver2-template-on-the-hdp-cluster">Install the Live Hiveserver2 template on the HDP cluster</a></li><li><a href="#activate-fusion-client-on-the-hdp-cluster">Activate Fusion Client on the HDP cluster</a></li><li><a href="#activate-live-hive-proxy-on-the-hdp-cluster">Activate Live Hive Proxy on the HDP cluster</a></li><li><a href="#restart-required-services">Restart required services</a></li><li><a href="#setup-databricks-on-adls-gen2-zone">Setup Databricks on ADLS Gen2 zone</a></li></ul></li><li><a href="#replication">Replication</a><ul class="toc-headings"><li><a href="#create-replication-rules">Create replication rules</a></li><li><a href="#test-replication">Test replication</a></li><li><a href="#verify-replication">Verify replication</a></li></ul></li><li><a href="#troubleshooting">Troubleshooting</a><ul class="toc-headings"><li><a href="#error-relating-to-system_dbus_socket">Error relating to 'system_dbus_socket'</a></li><li><a href="#error-relating-to-connection-refused-after-starting-fusion-for-the-first-time">Error relating to 'connection refused' after starting Fusion for the first time</a></li><li><a href="#fusion-zones-not-inducted-together-after-initial-start-up">Fusion zones not inducted together after initial start-up</a></li></ul></li><li><a href="#advanced-options">Advanced options</a></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="https://wandisco.github.io/wandisco-documentation/" class="nav-home"></a><div><h5>Docs</h5><a href="https://wandisco.github.io/wandisco-documentation/docs/en/quickstarts/quickstart-config.html">Getting Started</a><a href="https://docs.wandisco.com">Product User Guides</a><a href="https://community.wandisco.com/s/knowledge-base">Knowledge Base</a></div><div><h5>Community</h5><a href="https://community.wandisco.com/">WANdisco Community</a><a href="//wandisco.com/partners/find">Partners</a><a href="https://twitter.com/" target="_blank" rel="noreferrer noopener">Twitter</a></div><div><h5>More</h5><a href="https://wandisco.com/blog">Blog</a><a href="https://blogs.wandisco.com/">Developer Blog</a><a href="https://github.com/wandisco">GitHub</a></div></section><section class="copyright">Copyright © 2019 WANdisco, Inc.</section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>
                document.addEventListener('keyup', function(e) {
                  if (e.target !== document.body) {
                    return;
                  }
                  // keyCode for '/' (slash)
                  if (e.keyCode === 191) {
                    const search = document.getElementById('search_input_react');
                    search && search.focus();
                  }
                });
              </script><script>
              var search = docsearch({
                
                apiKey: '56e573413aa88e9ec072a585bec45683',
                indexName: 'wandisco',
                inputSelector: '#search_input_react',
                algoliaOptions: {}
              });
            </script></body></html>