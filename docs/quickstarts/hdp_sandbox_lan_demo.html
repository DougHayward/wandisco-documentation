<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Hortonworks (HDP) Sandbox to Azure Databricks with LiveAnalytics · WANdisco docs</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="_THIS GUIDE IS WORK IN PROGRESS, PLEASE DO NOT FOLLOW ANYTHING HERE UNTIL THIS WARNING IS REMOVED_"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Hortonworks (HDP) Sandbox to Azure Databricks with LiveAnalytics · WANdisco docs"/><meta property="og:type" content="website"/><meta property="og:url" content="https://wandisco.github.io/wandisco-documentation/ https://wandisco.github.io/wandisco-documentation/"/><meta property="og:description" content="_THIS GUIDE IS WORK IN PROGRESS, PLEASE DO NOT FOLLOW ANYTHING HERE UNTIL THIS WARNING IS REMOVED_"/><meta property="og:image" content="https://wandisco.github.io/wandisco-documentation/ https://wandisco.github.io/wandisco-documentation/img/undraw_online.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://wandisco.github.io/wandisco-documentation/ https://wandisco.github.io/wandisco-documentation/img/undraw_tweetstorm.svg"/><link rel="shortcut icon" href="https://wandisco.github.io/wandisco-documentation/img/favicon.png"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script src="https://wandisco.github.io/wandisco-documentation/js/scrollSpy.js"></script><link rel="stylesheet" href="https://wandisco.github.io/wandisco-documentation/css/main.css"/><script src="https://wandisco.github.io/wandisco-documentation/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="https://wandisco.github.io/wandisco-documentation/"><img class="logo" src="https://wandisco.github.io/wandisco-documentation/img/favicon.png" alt="WANdisco docs"/><h2 class="headerTitleWithLogo">WANdisco docs</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="https://wandisco.github.io/wandisco-documentation/docs/quickstarts/installation/quickstart-config" target="_self">Quickstarts</a></li><li class=""><a href="https://wandisco.github.io/wandisco-documentation/docs/docs/doc1" target="_self">Docs</a></li><li class=""><a href="https://wandisco.github.io/wandisco-documentation/docs/glossary/a" target="_self">Glossary</a></li><li class=""><a href="https://wandisco.github.io/wandisco-documentation/docs/api/api" target="_self">API</a></li><li class=""><a href="https://wandisco.github.io/wandisco-documentation/docs/help/need_help" target="_self">Help</a></li><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search" title="Search"/></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="container mainContainer"><div class="wrapper"><div class="post"><header class="postHeader"><h1 class="postHeaderTitle">Hortonworks (HDP) Sandbox to Azure Databricks with LiveAnalytics</h1></header><article><div><span><p><em>THIS GUIDE IS WORK IN PROGRESS, PLEASE DO NOT FOLLOW ANYTHING HERE UNTIL THIS WARNING IS REMOVED</em></p>
<p>Use this quickstart if you want to configure Fusion to replicate from a Hortonworks (HDP) Sandbox to an Azure Databricks cluster.</p>
<p>This will involve the use of Live Hive for the HDP cluster, and the Databricks Delta Lake plugin for the Azure Databricks cluster. These two products form the LiveAnalytics solution.</p>
<p>What this guide will cover:</p>
<ul>
<li>Installing WANdisco Fusion and the HDP sandbox using the <a href="https://docs.docker.com/compose/">docker-compose</a> tool.</li>
<li>Integrating WANdisco Fusion with Azure Databricks.</li>
<li>Performing a sample data migration.</li>
</ul>
<p>Please see the <a href="https://wandisco.github.io/wandisco-documentation/docs/quickstarts/hdp_sandbox_fusion_stop_start">shutdown and start up</a> guide for when you wish to safely shutdown or start back up the installation.</p>
<h2><a class="anchor" aria-hidden="true" id="prerequisites"></a><a href="#prerequisites" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Prerequisites</h2>
<p>To complete this quickstart, you will need:</p>
<ul>
<li>Azure VM created and started. See the <a href="https://wandisco.github.io/wandisco-documentation/docs/quickstarts/preparation/azure_vm_creation">Azure VM creation</a> guide for steps to create an Azure VM.
<ul>
<li>Minimum size VM recommendation = <strong>Standard D8 v3 (8 vcpus, 32 GiB memory).</strong></li>
<li>CentOS-based 7.7 (or higher) or UbuntuLTS 18.04. Instructions are provided for these releases.</li>
<li>A minimum of 128GB storage. The <a href="https://wandisco.github.io/wandisco-documentation/docs/quickstarts/preparation/azure_vm_creation">Azure VM creation</a> guide includes this by default.</li>
<li>Root access on server (this is normally available by default).</li>
</ul></li>
<li>Azure VM prepared for Fusion installation, see <a href="https://wandisco.github.io/wandisco-documentation/docs/quickstarts/preparation/azure_vm_prep">Azure VM preparation</a> guide for all required steps.</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="installation"></a><a href="#installation" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Installation</h2>
<p>Please log into your VM prior to starting these steps. All the commands within this guidance should be run as <strong>root</strong> user.</p>
<h3><a class="anchor" aria-hidden="true" id="setup-fusion"></a><a href="#setup-fusion" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Setup Fusion</h3>
<ol>
<li><p>Clone the Fusion docker repository to your Azure VM instance:</p>
<p><code>git clone -b features/hdp-sandbox https://github.com/WANdisco/fusion-docker-compose.git</code></p></li>
<li><p>Change to the repository directory:</p>
<p><code>cd fusion-docker-compose</code></p></li>
<li><p>Run the setup script:</p>
<p><code>./setup-env.sh</code></p></li>
<li><p>Enter <code>y</code> when asked whether to use the HDP sandbox.</p></li>
<li><p>Follow the prompts to configure your ADLS Gen2 Zone, see the next section below for guidance on this.</p></li>
</ol>
<h4><a class="anchor" aria-hidden="true" id="setup-prompts-for-adls-gen2"></a><a href="#setup-prompts-for-adls-gen2" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Setup prompts for ADLS Gen2</h4>
<p>Please ensure to enter your details for the <strong>Storage account</strong>, <strong>Storage container</strong> and <strong>Account Key</strong> values so that they match your account in Azure.
The examples shown below are for guidance only.</p>
<ul>
<li><p>Storage account: <code>adlsg2storage</code></p></li>
<li><p>Storage container: <code>fusionreplication</code></p></li>
<li><p>Account key: <code>KEY_1_STRING</code> - the Primary Access Key is now referred to as &quot;Key1&quot; in Microsoft’s documentation. You can get the Access Key from the Microsoft Azure storage account under the <strong>Access Keys</strong> section.</p></li>
<li><p>default FS: <code>abfss://fusionreplication@adlsg2storage.dfs.core.windows.net/</code> - press enter for the default value.</p></li>
<li><p>underlying FS: <code>abfs://fusionreplication@adlsg2storage.dfs.core.windows.net/</code> - press enter for the default value.</p></li>
</ul>
<p>At this point, the setup prompts will be complete and the script will exit out with an informational message.</p>
<h3><a class="anchor" aria-hidden="true" id="startup-fusion"></a><a href="#startup-fusion" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Startup Fusion</h3>
<p>After all the prompts have been completed, you will be able to start the containers.</p>
<ol>
<li><p>Ensure that Docker is started:</p>
<p><code>systemctl status docker</code></p>
<p>If not, start the Docker service:</p>
<p><code>systemctl start docker</code></p></li>
<li><p>Start the Fusion containers with:</p>
<p><code>docker-compose up -d</code></p></li>
</ol>
<p>Docker will now download all required images and start the containers, please wait until this is completed (~10mins).</p>
<h2><a class="anchor" aria-hidden="true" id="configuration"></a><a href="#configuration" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Configuration</h2>
<h3><a class="anchor" aria-hidden="true" id="live-hive-configuration-and-activation"></a><a href="#live-hive-configuration-and-activation" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Live Hive configuration and activation</h3>
<ol>
<li><p>Log into one of the containers for the HDP zone.</p>
<p><code>docker exec -it fusion_fusion-ui-server-hdp_1 bash</code></p></li>
</ol>
<ol start="2">
<li><p>Add an additional property to the Live Hive config:</p>
<p><code>vi /etc/wandisco/fusion/plugins/hive/live-hive-site.xml</code></p>
<p>Add the following property and value below:</p>
<pre><code class="hljs css language-json">  &lt;property&gt;
    &lt;name&gt;live.hive.cluster.delegation.token.delayed.removal.interval.in.seconds&lt;/name&gt;
    &lt;value&gt;5&lt;/value&gt;
  &lt;/property&gt;
</code></pre>
<p>Once complete, save and quit the file (e.g. <code>:wq</code>).</p></li>
<li><p>Exit back into the docker host and restart the Fusion containers so that the configuration changes are picked up.</p>
<p><code>exit</code></p>
<p><code>docker-compose restart</code></p></li>
<li><p>Log into the Fusion UI for the HDP zone, and activate the Live Hive plugin.</p>
<p><code>http://&lt;docker_IP_address&gt;:8083</code></p>
<p>Username: <code>admin</code>
Password: <code>admin</code></p>
<p>Proceed to the Settings tab and select the <em>Live Hive: Plugin Activation</em> option on the left-hand panel.</p>
<p>Click on the <em>Activate</em> option. Wait for the <strong>Reload this window</strong> message to appear and refresh the page.</p></li>
</ol>
<h3><a class="anchor" aria-hidden="true" id="setup-databricks"></a><a href="#setup-databricks" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Setup Databricks</h3>
<p>Prior to performing these tasks, the Databricks cluster must be in a <strong>running</strong> state. Please access the Azure portal and check the status of the cluster. If it is not running, select to start the cluster and wait until it is <strong>running</strong> before continuing.</p>
<ol>
<li><p>Log into the Fusion UI for the ADLS Gen2 zone.</p>
<p><code>http://&lt;docker_IP_address&gt;:8583</code></p>
<p>Username: <code>admin</code>
Password: <code>admin</code></p></li>
<li><p>Enter the Databricks Configuration details on the Settings page.</p>
<p><strong>Fusion UI -&gt; Settings -&gt; Databricks: Configuration</strong></p>
<p><em>Examples for Databricks details</em></p>
<ul>
<li><p>Databricks Service Address: <code>westeurope.azuredatabricks.net</code></p></li>
<li><p>Bearer Token: <code>dapicd7689jkb25473c765ghty78bb299a83</code></p></li>
<li><p>Databricks Cluster ID: <code>2233-255452-boned277</code></p></li>
<li><p>Unique JDBC HTTP path: <code>sql/protocolv1/o/6987013384345789/2233-255452-boned277</code></p></li>
</ul>
<p><em>Ensure to change the above examples to match your Databricks details.</em></p>
<p>Click <strong>Update</strong> once complete.</p></li>
<li><p>On the docker host, log into one of the containers for the ADLS Gen2 zone.</p>
<p><code>docker exec -it fusion_fusion-server-adls2_1 bash</code></p></li>
</ol>
<ol start="4">
<li><p>Upload the LiveAnalytics &quot;datatransformer&quot; jar using a curl command.</p>
<p><code>curl -v -H &quot;Authorization: Bearer &lt;bearer_token&gt;&quot; -F contents=@/opt/wandisco/fusion/plugins/databricks/live-analytics-databricks-etl-5.0.0.0.jar -F path=&quot;/datatransformer.jar&quot; https://&lt;databricks_service_address&gt;/api/2.0/dbfs/put</code></p>
<p>You will need to adjust the <code>curl</code> command so that your <strong>Bearer Token</strong> and <strong>Databricks Service Address</strong> is referenced.</p>
<p><em>Example values</em></p>
<ul>
<li>Bearer Token: <code>dapicd7689jkb25473c765ghty78bb299a83</code></li>
<li>Databricks Service Address: <code>westeurope.azuredatabricks.net</code></li>
</ul>
<p><em>Example command</em></p>
<p><code>curl -v -H &quot;Authorization: Bearer dapicd7689jkb25473c765ghty78bb299a83&quot; -F contents=@/opt/wandisco/fusion/plugins/databricks/live-analytics-databricks-etl-5.0.0.0.jar -F path=&quot;/datatransformer.jar&quot; https://westeurope.azuredatabricks.net/api/2.0/dbfs/put</code></p>
<p>If the command is successful, you will see that the message output contains the following text below:</p>
<pre><code class="hljs css language-json">&lt; HTTP/1.1 100 Continue
&lt; HTTP/1.1 200 OK
</code></pre></li>
<li><p>Exit back into the docker host and restart the Fusion containers so that the configuration changes are picked up.</p>
<p><code>exit</code></p>
<p><code>docker-compose restart</code></p></li>
<li><p>Log into the Azure portal and Launch Workspace for your Databricks cluster.</p></li>
<li><p>On the left-hand panel, select <strong>Clusters</strong> and then select your interactive cluster.</p></li>
<li><p>Click on the <strong>Libraries</strong> tab, and select the option to <strong>Install New</strong>.</p></li>
<li><p>Select the following options for the Install Library prompt:</p>
<ul>
<li><p>Library Source = <code>DBFS</code></p></li>
<li><p>Library Type = <code>Jar</code></p></li>
<li><p>File Path = <code>dbfs:/datatransformer.jar</code></p></li>
</ul></li>
<li><p>Select <strong>Install</strong> once the details are entered. Wait for the <strong>Status</strong> of the jar to display as <strong>Installed</strong> before continuing.</p></li>
</ol>
<h2><a class="anchor" aria-hidden="true" id="replication"></a><a href="#replication" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Replication</h2>
<p>In this section, follow the steps detailed to perform live replication of HCFS data and Hive metadata from the HDP cluster to the Azure Databricks cluster.</p>
<h3><a class="anchor" aria-hidden="true" id="create-replication-rules"></a><a href="#create-replication-rules" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Create replication rules</h3>
<ol>
<li><p>Log into the Fusion UI for the HDP zone.</p>
<p><code>http://&lt;docker_IP_address:8083</code></p>
<p>Username: <code>admin</code>
Password: <code>admin</code></p></li>
<li><p>Enter the Replication tab, and select to <strong>+ Create</strong> a replication rule.</p></li>
<li><p>Create a new HCFS rule using the UI with the following properties:</p>
<ul>
<li><p>Type = <code>HCFS</code></p></li>
<li><p>Zones = <code>adls2, hdp</code> <em>- Leave as default.</em></p></li>
<li><p>Priority Zone = <code>hdp</code> <em>- Leave as default.</em></p></li>
<li><p>Rule Name = <code>warehouse</code></p></li>
<li><p>Path for adls2 = <code>/apps/hive/warehouse</code></p></li>
<li><p>Path for hdp = <code>/apps/hive/warehouse</code></p></li>
</ul>
<p>Click <strong>Add</strong> after entering the Rule Name and Paths.</p>
<ul>
<li>Advanced Options: Preserve Origin Block Size = <code>true</code> <em>- click the checkbox to set this to true.</em></li>
</ul>
<p>Click <strong>Create rules (1)</strong> once complete.</p></li>
<li><p>Create a new Hive rule using the UI with the following properties:</p>
<p>On the Replication tab, select to <strong>+ Create</strong> a replication rule again.</p>
<ul>
<li><p>Type = <code>Hive</code></p></li>
<li><p>Database name = <code>test*</code></p></li>
<li><p>Table name = <code>*</code></p></li>
<li><p>Description = <code>Testing</code> <em>- this field is optional</em></p></li>
</ul>
<p>Click <strong>Create rule</strong> once complete.</p></li>
<li><p>Both rules should now display on the <strong>Replication</strong> tab in the Fusion UI.</p></li>
</ol>
<h3><a class="anchor" aria-hidden="true" id="test-replication"></a><a href="#test-replication" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Test replication</h3>
<p>Prior to performing these tasks, the Databricks cluster must be in a <strong>running</strong> state. Please access the Azure portal and check the status of the cluster. If it is not running, select to start the cluster and wait until it is <strong>running</strong> before continuing.</p>
<ol>
<li><p>On the docker host, log into the HDP cluster node.</p>
<p><code>docker exec -it sandbox-hdp bash</code></p></li>
<li><p>Run beeline and use the <code>!connect</code> string to start a Hive session via the Hiveserver2 service.</p>
<p><code>beeline</code></p>
<p><code>beeline&gt; !connect jdbc:hive2://sandbox-hdp:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2</code></p>
<p>This connection string can also be found on the Ambari UI under <strong>Hive -&gt; Summary -&gt; HIVESERVER2 JDBC URL</strong>.</p>
<p>When prompted for a username and password, enter the following:</p>
<p><code>Enter username: hdfs</code></p>
<p><code>Enter password:</code> <em>- leave blank and press enter.</em></p></li>
<li><p>Create a database to use that will match the regex for the Hive replication rule created earlier in the Fusion UI.</p>
<p><code>0: jdbc:hive2://sandbox-hdp:2181/&gt; create database test01;</code></p></li>
<li><p>Create a table inside of the database.</p>
<p><code>0: jdbc:hive2://sandbox-hdp:2181/&gt; use test01;</code></p>
<p><code>0: jdbc:hive2://sandbox-hdp:2181/&gt; create table table01(id int, name string) stored as ORC;</code></p></li>
<li><p>Insert data inside of the table.</p>
<p><code>0: jdbc:hive2://sandbox-hdp:2181/&gt; insert into table01 values (1,'words');</code></p>
<p>This will now launch a Hive job that will insert the data values provided in this example. If this is successful, you will see <strong>SUCCEEDED</strong> written in the STATUS column.</p>
<p><em>Example</em></p>
<pre><code class="hljs css language-json">--------------------------------------------------------------------------------
        VERTICES      STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED
--------------------------------------------------------------------------------
Map 1 ..........   SUCCEEDED      1          1        0        0       0       0
--------------------------------------------------------------------------------
VERTICES: 01/01  [==========================&gt;&gt;] 100%  ELAPSED TIME: XY.ZA s
--------------------------------------------------------------------------------
</code></pre>
<p>Please note that running an 'insert into table' for the first time on the HDP cluster may take a longer period of time than normal. Further jobs will complete at a much faster rate.</p></li>
</ol>
<h3><a class="anchor" aria-hidden="true" id="verify-replication"></a><a href="#verify-replication" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Verify replication</h3>
<ol>
<li><p>To verify the data values inside of the table on the <strong>HDP</strong> zone, run the command below when still logged into the Hive beeline session:</p>
<p><code>0: jdbc:hive2://sandbox-hdp:2181/&gt; select * from table01;</code></p>
<p>The output will be similar to that of below:</p>
<pre><code class="hljs css language-json">+-------------+----------------+--+
| table01.id  |  table01.name  |
+-------------+----------------+--+
| 1           | words          |
+-------------+----------------+--+
1 rows selected (X.YZA seconds)
</code></pre></li>
<li><p>To verify the data has replicated to the ADLS Gen2 zone and Databricks cluster, access the Azure portal and and Launch Workspace for your Databricks cluster (if not already opened).</p></li>
<li><p>On the left-hand panel, select <strong>Data</strong> and then select the database created for this test (i.e. <code>test01</code>).</p></li>
<li><p>In the <em>Tables</em> list, select the table created for this test (i.e. <code>table01</code>).</p></li>
<li><p>Wait for the table details to be loaded, and verify that the Schema and Sample Data match that seen in the HDP zone.</p>
<p><strong>Schema</strong></p>
<pre><code class="hljs css language-json">col_name   data_type
id         int
name       string
</code></pre>
<p><strong>Sample Data</strong></p>
<pre><code class="hljs css language-json">id         name
1          words
</code></pre></li>
</ol>
<h2><a class="anchor" aria-hidden="true" id="troubleshooting"></a><a href="#troubleshooting" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Troubleshooting</h2>
<p>Please see the <a href="https://wandisco.github.io/wandisco-documentation/docs/quickstarts/troubleshooting/useful_info">Useful information</a> section for additional commands and help.</p>
<h3><a class="anchor" aria-hidden="true" id="error-connection-refused-after-starting-fusion-for-the-first-time"></a><a href="#error-connection-refused-after-starting-fusion-for-the-first-time" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Error 'connection refused' after starting Fusion for the first time</h3>
<p>You may see the following error occur when running <code>docker-compose up -d</code> for the first time inside the fusion-docker-compose repository:</p>
<pre><code class="hljs css language-json">ERROR: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on [::1]:53: read udp [::1]:52155-&gt;[::1]:53: read: connection refused
</code></pre>
<p>If encountering this error, run the <code>docker-compose up -d</code> command again, and this should initiate the download of the docker images.</p>
<h3><a class="anchor" aria-hidden="true" id="fusion-zones-not-inducted-together"></a><a href="#fusion-zones-not-inducted-together" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Fusion zones not inducted together</h3>
<p>If the Fusion zones are not inducted together after starting Fusion for the first time (<code>docker-compose up -d</code>), you can simply run the same command again to start the induction container:</p>
<p><code>docker-compose up -d</code></p>
<h3><a class="anchor" aria-hidden="true" id="hiveserver2-down-after-hdp-sandbox-is-started"></a><a href="#hiveserver2-down-after-hdp-sandbox-is-started" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Hiveserver2 down after HDP Sandbox is started</h3>
<p>The Hiveserver2 component in the HDP sandbox may be down after starting the cluster. If so, try the following steps to start it back up.</p>
<ol>
<li><p>On the docker host, change directory to the Fusion docker compose directory and restart the Fusion Server container for the HDP zone.</p>
<p><code>cd /path/to/fusion-docker-compose</code></p>
<p><code>docker-compose restart fusion-server-hdp</code></p>
<p>Wait until the container has finished restarting before continuing.</p></li>
<li><p>Access the Ambari UI, and manually start the Hiveserver2 component.</p>
<p><strong>Ambari UI -&gt; Hive -&gt; Summary -&gt; Click on the &quot;HIVESERVER2&quot; written in blue text.</strong></p></li>
<li><p>Locate the HiveServer2 in the component list and click the <code>...</code> in the Action column. Select to <strong>Start</strong> the component in the drop-down list.</p></li>
</ol>
<h3><a class="anchor" aria-hidden="true" id="spark2-history-server-down-after-hdp-sandbox-is-started-for-first-time"></a><a href="#spark2-history-server-down-after-hdp-sandbox-is-started-for-first-time" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Spark2 History Server down after HDP Sandbox is started for first time</h3>
<p>When starting the HDP Sandbox for the first time, the Spark2 History Server may be in a stopped state. This is often due to the order in which Spark2 and the WANdisco Fusion client is installed.</p>
<p>To resolve and bring the History Server online, follow the steps below:</p>
<ol>
<li><p>In the Ambari UI, select to Refresh configs for the WANdisco Fusion service.</p>
<p><strong>Ambari UI -&gt; WANdisco Fusion -&gt; Actions -&gt; Refresh configs -&gt; OK</strong></p></li>
<li><p>Start the Spark2 service.</p>
<p><strong>Ambari UI -&gt; Spark2 -&gt; Actions -&gt; Start -&gt; CONFIRM START</strong></p></li>
</ol>
<h2><a class="anchor" aria-hidden="true" id="advanced-options"></a><a href="#advanced-options" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Advanced options</h2>
<ul>
<li>This guide does not currently offer configuration of Fusion to a <strong>Kerberized</strong> HDP cluster.</li>
<li>This guide does not currently offer configuration of Fusion to a NameNode HA HDP cluster.</li>
</ul>
<p>Please contact <a href="https://wandisco.com/contact">WANdisco</a> for further information on Fusion with docker.</p>
</span></div></article></div><div class="docs-prevnext"></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#prerequisites">Prerequisites</a></li><li><a href="#installation">Installation</a><ul class="toc-headings"><li><a href="#setup-fusion">Setup Fusion</a></li><li><a href="#startup-fusion">Startup Fusion</a></li></ul></li><li><a href="#configuration">Configuration</a><ul class="toc-headings"><li><a href="#live-hive-configuration-and-activation">Live Hive configuration and activation</a></li><li><a href="#setup-databricks">Setup Databricks</a></li></ul></li><li><a href="#replication">Replication</a><ul class="toc-headings"><li><a href="#create-replication-rules">Create replication rules</a></li><li><a href="#test-replication">Test replication</a></li><li><a href="#verify-replication">Verify replication</a></li></ul></li><li><a href="#troubleshooting">Troubleshooting</a><ul class="toc-headings"><li><a href="#error-connection-refused-after-starting-fusion-for-the-first-time">Error 'connection refused' after starting Fusion for the first time</a></li><li><a href="#fusion-zones-not-inducted-together">Fusion zones not inducted together</a></li><li><a href="#hiveserver2-down-after-hdp-sandbox-is-started">Hiveserver2 down after HDP Sandbox is started</a></li><li><a href="#spark2-history-server-down-after-hdp-sandbox-is-started-for-first-time">Spark2 History Server down after HDP Sandbox is started for first time</a></li></ul></li><li><a href="#advanced-options">Advanced options</a></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="https://wandisco.github.io/wandisco-documentation/" class="nav-home"></a><div><h5>Docs</h5><a href="https://wandisco.github.io/wandisco-documentation/docs/en/quickstarts/quickstart-config.html">Getting Started</a><a href="https://docs.wandisco.com">Product User Guides</a><a href="https://community.wandisco.com/s/knowledge-base">Knowledge Base</a></div><div><h5>Community</h5><a href="https://community.wandisco.com/">WANdisco Community</a><a href="//wandisco.com/partners/find">Partners</a><a href="https://twitter.com/" target="_blank" rel="noreferrer noopener">Twitter</a></div><div><h5>More</h5><a href="https://wandisco.com/blog">Blog</a><a href="https://blogs.wandisco.com/">Developer Blog</a><a href="https://github.com/wandisco">GitHub</a></div></section><section class="copyright">Copyright © 2020 WANdisco, Inc.</section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>
                document.addEventListener('keyup', function(e) {
                  if (e.target !== document.body) {
                    return;
                  }
                  // keyCode for '/' (slash)
                  if (e.keyCode === 191) {
                    const search = document.getElementById('search_input_react');
                    search && search.focus();
                  }
                });
              </script><script>
              var search = docsearch({
                
                apiKey: '56e573413aa88e9ec072a585bec45683',
                indexName: 'wandisco',
                inputSelector: '#search_input_react',
                algoliaOptions: {}
              });
            </script></body></html>