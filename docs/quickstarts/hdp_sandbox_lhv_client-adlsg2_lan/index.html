<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Hortonworks (HDP) Sandbox to Azure Databricks with LiveAnalytics · WANdisco docs</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="Use this quickstart if you want to configure Fusion to replicate from a non-kerberized Hortonworks (HDP) Sandbox to an Azure Databricks cluster."/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Hortonworks (HDP) Sandbox to Azure Databricks with LiveAnalytics · WANdisco docs"/><meta property="og:type" content="website"/><meta property="og:url" content="https://wandisco.github.io/wandisco-documentation/ https://wandisco.github.io/wandisco-documentation/"/><meta property="og:description" content="Use this quickstart if you want to configure Fusion to replicate from a non-kerberized Hortonworks (HDP) Sandbox to an Azure Databricks cluster."/><meta property="og:image" content="https://wandisco.github.io/wandisco-documentation/ https://wandisco.github.io/wandisco-documentation/img/undraw_online.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://wandisco.github.io/wandisco-documentation/ https://wandisco.github.io/wandisco-documentation/img/undraw_tweetstorm.svg"/><link rel="shortcut icon" href="https://wandisco.github.io/wandisco-documentation/img/favicon.png"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script src="https://wandisco.github.io/wandisco-documentation/js/scrollSpy.js"></script><link rel="stylesheet" href="https://wandisco.github.io/wandisco-documentation/css/main.css"/><script src="https://wandisco.github.io/wandisco-documentation/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="https://wandisco.github.io/wandisco-documentation/"><img class="logo" src="https://wandisco.github.io/wandisco-documentation/img/favicon.png" alt="WANdisco docs"/><h2 class="headerTitleWithLogo">WANdisco docs</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="https://wandisco.github.io/wandisco-documentation/docs/quickstarts/installation/quickstart-config" target="_self">Quickstarts</a></li><li class=""><a href="https://wandisco.github.io/wandisco-documentation/docs/docs/doc1" target="_self">Docs</a></li><li class=""><a href="https://wandisco.github.io/wandisco-documentation/docs/glossary/a" target="_self">Glossary</a></li><li class=""><a href="https://wandisco.github.io/wandisco-documentation/docs/api/api" target="_self">API</a></li><li class=""><a href="https://wandisco.github.io/wandisco-documentation/docs/help/need_help" target="_self">Help</a></li><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search" title="Search"/></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="container mainContainer"><div class="wrapper"><div class="post"><header class="postHeader"><h1 class="postHeaderTitle">Hortonworks (HDP) Sandbox to Azure Databricks with LiveAnalytics</h1></header><article><div><span><p>Use this quickstart if you want to configure Fusion to replicate from a non-kerberized Hortonworks (HDP) Sandbox to an Azure Databricks cluster.</p>
<p>This will involve the use of Live Hive for the HDP cluster, and the Databricks Delta Lake plugin for the Azure Databricks cluster. These two products form the LiveAnalytics solution.</p>
<p>What this guide will cover:</p>
<ul>
<li>Installing WANdisco Fusion using the <a href="https://docs.docker.com/compose/">docker-compose</a> tool.</li>
<li>Integrating WANdisco Fusion with Azure Databricks.</li>
<li>Performing a sample data migration.</li>
</ul>
<p>Please see the <a href="https://wandisco.github.io/wandisco-documentation/docs/quickstarts/hdp_sandbox_fusion_stop_start">shutdown and start up</a> guide for when you wish to safely shutdown or start back up the environment.</p>
<h2><a class="anchor" aria-hidden="true" id="prerequisites"></a><a href="#prerequisites" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Prerequisites</h2>
<p>To complete this quickstart, you will need:</p>
<ul>
<li>Azure VM created and started, matching the following specifications:
<ul>
<li>Minimum size VM recommendation = <strong>Standard D4 v3 (4 vcpus, 16 GiB memory).</strong></li>
<li>A minimum of 32GB Temp storage for the <code>/var/lib/docker</code> directory.</li>
<li>Root access on server (this is normally available by default).</li>
</ul></li>
</ul>
<p>If seeking guidance on how to create a suitable VM with all utilities installed, see our <a href="https://wandisco.github.io/wandisco-documentation/docs/quickstarts/preparation/azure_vm_creation">Azure VM creation</a> guide.</p>
<ul>
<li>The following utilities must be installed on the server:
<ul>
<li><a href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git">Git</a></li>
<li><a href="https://docs.docker.com/install/">Docker</a> (v19.03.5 or higher)</li>
<li><a href="https://docs.docker.com/compose/install/#install-compose">Docker Compose for Linux</a> (v1.25.0 or higher)</li>
</ul></li>
</ul>
<p>If seeking guidance on how to install these utilities, see our <a href="https://wandisco.github.io/wandisco-documentation/docs/quickstarts/preparation/azure_vm_prep">Azure VM preparation</a> guide. This is not required if you have used our <a href="https://wandisco.github.io/wandisco-documentation/docs/quickstarts/preparation/azure_vm_creation">Azure VM creation</a> guide as all utilities will have been included.</p>
<p><em>These instructions have been tested on Ubuntu LTS.</em></p>
<h2><a class="anchor" aria-hidden="true" id="installation"></a><a href="#installation" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Installation</h2>
<p>Please log into your VM prior to starting these steps. All the commands within this guidance should be run as <strong>root</strong> user.</p>
<h3><a class="anchor" aria-hidden="true" id="setup-fusion"></a><a href="#setup-fusion" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Setup Fusion</h3>
<ol>
<li><p>Clone the Fusion docker repository to your Azure VM instance:</p>
<p><code>git clone -b features/hdp-sandbox https://github.com/WANdisco/fusion-docker-compose.git</code></p></li>
<li><p>Change to the repository directory:</p>
<p><code>cd fusion-docker-compose</code></p></li>
<li><p>Run the setup script:</p>
<p><code>./setup-env.sh</code></p></li>
<li><p>Enter <code>y</code> when asked whether to use the HDP sandbox.</p></li>
<li><p>Follow the prompts to configure your ADLS Gen2 Zone, see the next section below for guidance on this.</p></li>
</ol>
<h4><a class="anchor" aria-hidden="true" id="setup-prompts-for-adls-gen2"></a><a href="#setup-prompts-for-adls-gen2" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Setup prompts for ADLS Gen2</h4>
<p>Please ensure to enter your details for the <strong>Storage account</strong>, <strong>Storage container</strong> and <strong>Account Key</strong> values so that they match your account in Azure.
The examples shown below are for guidance only.</p>
<ul>
<li><p>Storage account: <code>adlsg2storage</code></p></li>
<li><p>Storage container: <code>fusionreplication</code></p></li>
<li><p>Account key: <code>KEY_1_STRING</code> - the Primary Access Key is now referred to as &quot;Key1&quot; in Microsoft’s documentation. You can get the Access Key from the Microsoft Azure storage account under the <strong>Access Keys</strong> section.</p></li>
<li><p>default FS: <code>abfss://fusionreplication@adlsg2storage.dfs.core.windows.net/</code> - press enter for the default value.</p></li>
<li><p>underlying FS: <code>abfs://fusionreplication@adlsg2storage.dfs.core.windows.net/</code> - press enter for the default value.</p></li>
</ul>
<p>At this point, the setup prompts will be complete and the script will exit out with an informational message.</p>
<h3><a class="anchor" aria-hidden="true" id="startup-fusion"></a><a href="#startup-fusion" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Startup Fusion</h3>
<p>After all the prompts have been completed, you will be able to start the containers:</p>
<p><code>docker-compose up -d</code></p>
<p>Docker will now download all required images and create the containers, please wait until this is done.</p>
<h2><a class="anchor" aria-hidden="true" id="configuration"></a><a href="#configuration" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Configuration</h2>
<h3><a class="anchor" aria-hidden="true" id="install-liveanalytics-on-databricks-cluster"></a><a href="#install-liveanalytics-on-databricks-cluster" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Install LiveAnalytics on Databricks cluster</h3>
<p>Prior to performing these tasks, the Databricks cluster must be in a <strong>running</strong> state. Please access the Azure portal and check the status of the cluster. If it is not running, select to start the cluster and wait until it is <strong>running</strong>.</p>
<ol>
<li><p>Download LiveAnalytics Jar file from the Github repository:</p>
<p><a href="https://github.com/WANdisco/wandisco-documentation/raw/master/docs/quickstarts/resources/live-analytics-databricks-etl-6.0.0.1.jar">live-analytics-databricks-etl-6.0.0.1.jar</a></p>
<p>Save the file on your local machine.</p></li>
<li><p>In your Workspace for the Databricks cluster, on the left-hand panel, select <strong>Clusters</strong> and then select your interactive cluster.</p></li>
<li><p>Click on the <strong>Libraries</strong> tab, and select the option to <strong>Install New</strong>.</p></li>
<li><p>Select the following options for the Install Library prompt:</p>
<ul>
<li><p>Library Source = <code>Upload</code></p></li>
<li><p>Library Type = <code>Jar</code></p></li>
<li><p>File Path = Find save location of <code>datatransformer.jar</code> from step 1.</p></li>
</ul></li>
<li><p>Select <strong>Install</strong> once the details are entered. Wait for the <strong>Status</strong> of the jar to display as <strong>Installed</strong> before continuing.</p></li>
</ol>
<h3><a class="anchor" aria-hidden="true" id="check-hdp-services-are-started"></a><a href="#check-hdp-services-are-started" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Check HDP services are started</h3>
<p>The HDP sandbox services can take up to 5-10 minutes to start. You will need to ensure that the HDFS service is started before continuing.</p>
<ol>
<li><p>Log in to the Ambari UI via a web browser.</p>
<p><code>http://&lt;docker_IP_address&gt;:8080</code></p>
<p>Username: <code>admin</code>
Password: <code>admin</code></p></li>
<li><p>Select the <strong>HDFS</strong> service.</p></li>
<li><p>Wait until all the HDFS components are showing as <strong>Started</strong>.</p></li>
</ol>
<h3><a class="anchor" aria-hidden="true" id="live-hive-activation"></a><a href="#live-hive-activation" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Live Hive activation</h3>
<ol>
<li><p>Log into the Fusion UI for the HDP zone, and activate the Live Hive plugin.</p>
<p><code>http://&lt;docker_IP_address&gt;:8083</code></p>
<p>Username: <code>admin</code>
Password: <code>admin</code></p></li>
<li><p>Proceed to the Settings tab and select the <em>Live Hive: Plugin Activation</em> option on the left-hand panel.</p></li>
<li><p>Click on the <em>Activate</em> option. Wait for the <strong>Reload this window</strong> message to appear and refresh the page.</p></li>
</ol>
<h3><a class="anchor" aria-hidden="true" id="setup-databricks-in-fusion"></a><a href="#setup-databricks-in-fusion" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Setup Databricks in Fusion</h3>
<ol>
<li><p>Log into the Fusion UI for the ADLS Gen2 zone.</p>
<p><code>http://&lt;docker_IP_address&gt;:8583</code></p>
<p>Username: <code>admin</code>
Password: <code>admin</code></p></li>
<li><p>Enter the Databricks Configuration details on the Settings page.</p>
<p><strong>Fusion UI -&gt; Settings -&gt; Databricks: Configuration</strong></p>
<ul>
<li><p><a href="https://docs.databricks.com/dev-tools/databricks-connect.html#step-2-configure-connection-properties">Databricks Service Address</a></p></li>
<li><p><a href="https://docs.databricks.com/dev-tools/api/latest/authentication.html#generate-a-token">Bearer Token</a></p></li>
<li><p><a href="https://docs.databricks.com/workspace/workspace-details.html#cluster-url">Databricks Cluster ID</a></p></li>
<li><p><a href="https://docs.databricks.com/bi/jdbc-odbc-bi.html#construct-the-jdbc-url">Unique JDBC HTTP path</a></p></li>
</ul>
<p>Click <strong>Update</strong> once complete.</p></li>
</ol>
<h2><a class="anchor" aria-hidden="true" id="replication"></a><a href="#replication" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Replication</h2>
<p>Follow the steps detailed to perform live replication of HCFS data and Hive metadata from the HDP sandbox to the Azure Databricks cluster.</p>
<h3><a class="anchor" aria-hidden="true" id="create-replication-rules"></a><a href="#create-replication-rules" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Create replication rules</h3>
<ol>
<li><p>Log into the Fusion UI for the HDP zone.</p>
<p><code>http://&lt;docker_IP_address:8083</code></p>
<p>Username: <code>admin</code>
Password: <code>admin</code></p></li>
<li><p>Enter the Replication tab, and select to <strong>+ Create</strong> a replication rule.</p></li>
<li><p>Create a new HCFS rule using the UI with the following properties:</p>
<ul>
<li><p>Type = <code>HCFS</code></p></li>
<li><p>Zones = <code>adls2, sandbox-hdp</code> <em>- Leave as default.</em></p></li>
<li><p>Priority Zone = <code>sandbox-hdp</code> <em>- Leave as default.</em></p></li>
<li><p>Rule Name = <code>warehouse</code></p></li>
<li><p>Path for adls2 = <code>/apps/hive/warehouse</code></p></li>
<li><p>Path for hdp = <code>/apps/hive/warehouse</code></p></li>
</ul>
<p>Click <strong>Add</strong> after entering the Rule Name and Paths.</p>
<ul>
<li>Advanced Options: Preserve Origin Block Size = <code>true</code> <em>- click the checkbox to set this to true.</em></li>
</ul>
<p>Click <strong>Create rules (1)</strong> once complete.</p></li>
<li><p>Create a new Hive rule using the UI with the following properties:</p>
<p>On the Replication tab, select to <strong>+ Create</strong> a replication rule again.</p>
<ul>
<li><p>Type = <code>Hive</code></p></li>
<li><p>Database name = <code>databricks_demo</code></p></li>
<li><p>Table name = <code>*</code></p></li>
<li><p>Description = <code>Demo</code> <em>- this field is optional</em></p></li>
</ul>
<p>Click <strong>Create rule</strong> once complete.</p></li>
<li><p>Both rules should now display on the <strong>Replication</strong> tab in the Fusion UI.</p></li>
</ol>
<h3><a class="anchor" aria-hidden="true" id="test-replication"></a><a href="#test-replication" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Test replication</h3>
<p>Prior to performing these tasks, the Databricks cluster must be in a <strong>running</strong> state. Please access the Azure portal and check the status of the cluster. If it is not running, select to start the cluster and wait until it is <strong>running</strong>.</p>
<ol>
<li><p>On the <strong>Docker host</strong>:</p>
<p>a. Clone the sample-data repository.</p>
<p><code>git clone https://github.com/pivotalsoftware/pivotal-samples.git /tmp/pivotal-samples</code></p>
<p>b. Copy the contents of this repository into the <strong>fusion_sandbox-hdp_1</strong> container.</p>
<p><code>docker cp /tmp/pivotal-samples/ fusion_sandbox-hdp_1:/tmp/</code></p></li>
<li><p>Log in to the <strong>sandbox-hdp</strong> container and place data into HDFS.</p>
<p>a. Log in to the container.</p>
<p><code>docker-compose exec sandbox-hdp bash</code></p>
<p>b. Switch to the hdfs user.</p>
<p><code>sudo -iu hdfs</code></p>
<p>c. Change directory into the pivotal sample repository.</p>
<p><code>cd /tmp/pivotal-samples/sample-data</code></p>
<p>d. Create a directory within HDFS for the sample data.</p>
<p><code>hdfs dfs -mkdir -p /retail_demo/customer_addresses_dim_hive/</code></p>
<p>e. Place the sample data into HDFS, so that it can be accessed by Hive.</p>
<p><code>hdfs dfs -put customer_addresses_dim.tsv.gz /retail_demo/customer_addresses_dim_hive/</code></p></li>
<li><p>Run beeline and use the <code>!connect</code> string to start a Hive session via the Hiveserver2 service.</p>
<p><code>beeline</code></p>
<p><code>!connect jdbc:hive2://sandbox-hdp:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2</code></p>
<p>The above connection string can also be found on the Ambari UI under <strong>Hive -&gt; Summary -&gt; HIVESERVER2 JDBC URL</strong>.</p>
<p>When prompted for a username and password, enter the following:</p>
<p><code>Enter username: hdfs</code></p>
<p><code>Enter password:</code> <em>- leave blank and press enter.</em></p></li>
<li><p>Create a database to use that will store the sample data.</p>
<p><code>CREATE DATABASE IF NOT EXISTS retail_demo;</code></p></li>
<li><p>Create a table inside of the database that points to the data previously uploaded.</p>
<pre><code class="hljs css language-sql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> retail_demo.customer_addresses_dim_hive
(
Customer_Address_ID  <span class="hljs-built_in">bigint</span>,
Customer_ID          <span class="hljs-built_in">bigint</span>,
Valid_From_Timestamp <span class="hljs-built_in">timestamp</span>,
Valid_To_Timestamp   <span class="hljs-built_in">timestamp</span>,
House_Number         <span class="hljs-keyword">string</span>,
Street_Name          <span class="hljs-keyword">string</span>,
Appt_Suite_No        <span class="hljs-keyword">string</span>,
City                 <span class="hljs-keyword">string</span>,
State_Code           <span class="hljs-keyword">string</span>,
Zip_Code             <span class="hljs-keyword">string</span>,
Zip_Plus_Four        <span class="hljs-keyword">string</span>,
Country              <span class="hljs-keyword">string</span>,
Phone_Number         <span class="hljs-keyword">string</span>
)
<span class="hljs-keyword">ROW</span> <span class="hljs-keyword">FORMAT</span> <span class="hljs-keyword">DELIMITED</span> <span class="hljs-keyword">FIELDS</span> <span class="hljs-keyword">TERMINATED</span> <span class="hljs-keyword">BY</span> <span class="hljs-string">'\t'</span>
<span class="hljs-keyword">STORED</span> <span class="hljs-keyword">AS</span> TEXTFILE
LOCATION <span class="hljs-string">'/retail_demo/customer_addresses_dim_hive/'</span>;
</code></pre></li>
<li><p>Create a second database and table that will match the regex for the Hive replication rule created earlier in the Fusion UI.</p>
<p>a. Create Database:</p>
<p><code>CREATE DATABASE IF NOT EXISTS databricks_demo;</code></p>
<p>b. Create Table:</p>
<pre><code class="hljs css language-sql"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> databricks_demo.customer_addresses_dim_hive
(
Customer_Address_ID  <span class="hljs-built_in">bigint</span>,
Customer_ID          <span class="hljs-built_in">bigint</span>,
Valid_From_Timestamp <span class="hljs-built_in">timestamp</span>,
Valid_To_Timestamp   <span class="hljs-built_in">timestamp</span>,
House_Number         <span class="hljs-keyword">string</span>,
Street_Name          <span class="hljs-keyword">string</span>,
Appt_Suite_No        <span class="hljs-keyword">string</span>,
City                 <span class="hljs-keyword">string</span>,
State_Code           <span class="hljs-keyword">string</span>,
Zip_Code             <span class="hljs-keyword">string</span>,
Zip_Plus_Four        <span class="hljs-keyword">string</span>,
Country              <span class="hljs-keyword">string</span>,
Phone_Number         <span class="hljs-keyword">string</span>
)
<span class="hljs-keyword">stored</span> <span class="hljs-keyword">as</span> ORC;
</code></pre></li>
<li><p>Now insert data into the table above by running the following:</p>
<p><code>insert into databricks_demo.customer_addresses_dim_hive select * from retail_demo.customer_addresses_dim_hive where state_code ='CA';</code></p>
<p>This will now launch a Hive job that will insert the data values provided in this example. If this is successful, you will see <strong>SUCCEEDED</strong> written in the STATUS column.</p>
<pre><code class="hljs css language-json">--------------------------------------------------------------------------------
        VERTICES      STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED
--------------------------------------------------------------------------------
Map 1 ..........   SUCCEEDED      1          1        0        0       0       0
--------------------------------------------------------------------------------
VERTICES: 01/01  [==========================&gt;&gt;] 100%  ELAPSED TIME: X.YZ s
--------------------------------------------------------------------------------
</code></pre></li>
</ol>
<h3><a class="anchor" aria-hidden="true" id="setup-databricks-notebook-to-view-data"></a><a href="#setup-databricks-notebook-to-view-data" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Setup Databricks Notebook to view data</h3>
<ol>
<li><p>Return to your Workspace for the Databricks cluster.</p></li>
<li><p>Create a Cluster Notebook.</p>
<p><strong>Click Workspace on the left hand side &gt; click the drop down arrow &gt; Create &gt; Notebook</strong></p>
<ul>
<li>Name: <strong>WD-demo</strong></li>
<li>Language: <strong>SQL</strong></li>
<li>Cluster: (Choose the cluster used in this demo)</li>
</ul></li>
<li><p>You should now see a blank notebook.</p>
<p>a. Inside the 'Cmd 1' box, add the query:</p>
<p><code>select * from databricks_demo.customer_addresses_dim_hive;</code></p>
<p>b. Click 'Run Cell' (looks like a play button in the top right of that box).</p></li>
<li><p>Wait for the query to return, then select the drop-down graph type and choose <strong>Map</strong>.</p></li>
<li><p>Under the Plot Options &gt; remove all Keys &gt; click and drag 'state_code' from the 'All fields' box into the 'Keys' box.</p></li>
<li><p>Click Apply.</p></li>
<li><p>You should now see a plot of USA with colour shading - dependent on the population density.</p></li>
</ol>
<p>You have now completed this demo.</p>
<h2><a class="anchor" aria-hidden="true" id="troubleshooting"></a><a href="#troubleshooting" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Troubleshooting</h2>
<ul>
<li><p>Please see our <a href="https://wandisco.github.io/wandisco-documentation/docs/quickstarts/hdp_sandbox_lan_troubleshooting">Troubleshooting</a> guide for help with this demo.</p></li>
<li><p>Please contact <a href="https://wandisco.com/contact">WANdisco</a> for further information about Fusion.</p></li>
</ul>
</span></div></article></div><div class="docs-prevnext"></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#prerequisites">Prerequisites</a></li><li><a href="#installation">Installation</a><ul class="toc-headings"><li><a href="#setup-fusion">Setup Fusion</a></li><li><a href="#startup-fusion">Startup Fusion</a></li></ul></li><li><a href="#configuration">Configuration</a><ul class="toc-headings"><li><a href="#install-liveanalytics-on-databricks-cluster">Install LiveAnalytics on Databricks cluster</a></li><li><a href="#check-hdp-services-are-started">Check HDP services are started</a></li><li><a href="#live-hive-activation">Live Hive activation</a></li><li><a href="#setup-databricks-in-fusion">Setup Databricks in Fusion</a></li></ul></li><li><a href="#replication">Replication</a><ul class="toc-headings"><li><a href="#create-replication-rules">Create replication rules</a></li><li><a href="#test-replication">Test replication</a></li><li><a href="#setup-databricks-notebook-to-view-data">Setup Databricks Notebook to view data</a></li></ul></li><li><a href="#troubleshooting">Troubleshooting</a></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="https://wandisco.github.io/wandisco-documentation/" class="nav-home"></a><div><h5>Docs</h5><a href="https://wandisco.github.io/wandisco-documentation/docs/en/quickstarts/quickstart-config.html">Getting Started</a><a href="https://docs.wandisco.com">Product User Guides</a><a href="https://community.wandisco.com/s/knowledge-base">Knowledge Base</a></div><div><h5>Community</h5><a href="https://community.wandisco.com/">WANdisco Community</a><a href="//wandisco.com/partners/find">Partners</a><a href="https://twitter.com/" target="_blank" rel="noreferrer noopener">Twitter</a></div><div><h5>More</h5><a href="https://wandisco.com/blog">Blog</a><a href="https://blogs.wandisco.com/">Developer Blog</a><a href="https://github.com/wandisco">GitHub</a></div></section><section class="copyright">Copyright © 2020 WANdisco, Inc.</section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>
                document.addEventListener('keyup', function(e) {
                  if (e.target !== document.body) {
                    return;
                  }
                  // keyCode for '/' (slash)
                  if (e.keyCode === 191) {
                    const search = document.getElementById('search_input_react');
                    search && search.focus();
                  }
                });
              </script><script>
              var search = docsearch({
                
                apiKey: '56e573413aa88e9ec072a585bec45683',
                indexName: 'wandisco',
                inputSelector: '#search_input_react',
                algoliaOptions: {}
              });
            </script></body></html>